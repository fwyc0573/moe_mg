rank:8064:get_batch(stage_id=63,batch_id=0,mg_state=None,duration=0.46,description=simulation,group_kind=tp,input__shape=None,input__dtype=None,timestamp=3586077866.58,sub_operations=['trace_src_func=broadcast_wrapper,duration=0.0,timestamp=3586077866.3,input__shape=[1, 2048],input__dtype=torch.int64,func_name=labels,group=tp,comm_func=broadcast', 'trace_src_func=broadcast_wrapper,duration=0.02,timestamp=3586077866.42,input__shape=[1, 2048],input__dtype=torch.float32,func_name=loss_mask,group=tp,comm_func=broadcast', 'trace_src_func=broadcast_wrapper,duration=0.01,timestamp=3586077866.52,input__shape=[1, 1, 2048, 2048],input__dtype=torch.bool,func_name=attention_mask,group=tp,comm_func=broadcast'])
rank:8064:forward_step(stage_id=63,batch_id=0,mg_state=None,duration=7.23,description=simulation,group_kind=None,input__shape=None,input__dtype=None,timestamp=3586077873.89,sub_operations=['trace_src_func=allreduce,duration=0.0,timestamp=3586077868.69,input__shape=[2048, 20480],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=3586077871.84,input__shape=[2048, 20480],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=reduce_wrapper,duration=0.0,timestamp=3586077873.08,input__shape=[2048, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=reduce_wrapper,duration=0.08,timestamp=3586077873.51,input__shape=[2048, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=reduce_wrapper,duration=0.03,timestamp=3586077873.7,input__shape=[2048, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce'])
rank:8064:loss_func(stage_id=63,batch_id=0,mg_state=None,duration=0.28,description=simulation: loss_func, calculate and DP allreduce for the last stage,group_kind=None,input__shape=None,input__dtype=None,timestamp=3586077874.23,sub_operations=['trace_src_func=allreduce,duration=0.01,timestamp=3586077874.15,input__shape=[1],input__dtype=torch.float32,func_name=loss_func,group=dp,comm_func=allreduce'])
rank:8064:backward_step(stage_id=63,batch_id=0,mg_state=None,duration=10.6,description=simulation,group_kind=None,input__shape=None,input__dtype=None,timestamp=3586077884.97,sub_operations=['trace_src_func=allreduce_wrapper,duration=0.0,timestamp=3586077875.53,input__shape=[2048, 1, 20480],input__dtype=torch.float16,func_name=embedding_bwd_allreduce,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=3586077880.21,input__shape=[2048, 20480],input__dtype=torch.float16,func_name=normlinear_bwd_allreduce,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=3586077883.75,input__shape=[2048, 20480],input__dtype=torch.float16,func_name=normlinear_bwd_allreduce,group=tp,comm_func=allreduce'])
rank:8064:dp_allreduce(stage_id=63,batch_id=0,mg_state=None,duration=0.04,description=model_chunk.finish_grad_sync(), All-reduce / reduce-scatter across DP replicas,group_kind=dp,input__shape=[758105600],input__dtype=torch.float16,timestamp=3586077885.08,sub_operations=[])
rank:8064:ep_allreduce(stage_id=63,batch_id=0,mg_state=finalize,duration=0.02,description=Embedding parallel gradient synchronization,group_kind=ep,input__shape=[6288, 20480],input__dtype=torch.float16,timestamp=3586077885.17,sub_operations=[])
rank:8064:optimizer_step(stage_id=63,batch_id=0,mg_state=None,duration=14.25,description=simulation,group_kind=None,input__shape=None,input__dtype=None,timestamp=3586077899.45,sub_operations=[])
