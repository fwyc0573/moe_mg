rank:8064:get_batch(stage_id=63,batch_id=0,mg_state=None,duration=0.52,description=simulation,group_kind=tp,input__shape=None,input__dtype=None,timestamp=3587825897.23,sub_operations=['trace_src_func=broadcast_wrapper,duration=0.0,timestamp=3587825896.9,input__shape=[1, 2048],input__dtype=torch.int64,func_name=labels,group=tp,comm_func=broadcast', 'trace_src_func=broadcast_wrapper,duration=0.01,timestamp=3587825897.01,input__shape=[1, 2048],input__dtype=torch.float32,func_name=loss_mask,group=tp,comm_func=broadcast', 'trace_src_func=broadcast_wrapper,duration=0.01,timestamp=3587825897.09,input__shape=[1, 1, 2048, 2048],input__dtype=torch.bool,func_name=attention_mask,group=tp,comm_func=broadcast'])
rank:8064:forward_step(stage_id=63,batch_id=0,mg_state=None,duration=11.13,description=simulation,group_kind=None,input__shape=None,input__dtype=None,timestamp=3587825908.43,sub_operations=['trace_src_func=allreduce,duration=0.0,timestamp=3587825900.8,input__shape=[2048, 20480],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=3587825905.73,input__shape=[2048, 20480],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=reduce_wrapper,duration=0.0,timestamp=3587825907.55,input__shape=[2048, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=reduce_wrapper,duration=0.17,timestamp=3587825908.07,input__shape=[2048, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=reduce_wrapper,duration=0.0,timestamp=3587825908.27,input__shape=[2048, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce'])
rank:8064:loss_func(stage_id=63,batch_id=0,mg_state=None,duration=0.28,description=simulation: loss_func, calculate and DP allreduce for the last stage,group_kind=None,input__shape=None,input__dtype=None,timestamp=3587825908.79,sub_operations=['trace_src_func=allreduce,duration=0.01,timestamp=3587825908.69,input__shape=[1],input__dtype=torch.float32,func_name=loss_func,group=dp,comm_func=allreduce'])
rank:8064:backward_step(stage_id=63,batch_id=0,mg_state=None,duration=19.42,description=simulation,group_kind=None,input__shape=None,input__dtype=None,timestamp=3587825928.34,sub_operations=['trace_src_func=allreduce_wrapper,duration=0.0,timestamp=3587825910.79,input__shape=[2048, 1, 20480],input__dtype=torch.float16,func_name=embedding_bwd_allreduce,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=3587825919.47,input__shape=[2048, 20480],input__dtype=torch.float16,func_name=normlinear_bwd_allreduce,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=3587825926.2,input__shape=[2048, 20480],input__dtype=torch.float16,func_name=normlinear_bwd_allreduce,group=tp,comm_func=allreduce'])
rank:8064:dp_allreduce(stage_id=63,batch_id=0,mg_state=None,duration=0.02,description=model_chunk.finish_grad_sync(), All-reduce / reduce-scatter across DP replicas,group_kind=dp,input__shape=[1516047360],input__dtype=torch.float16,timestamp=3587825928.43,sub_operations=[])
rank:8064:ep_allreduce(stage_id=63,batch_id=0,mg_state=finalize,duration=0.02,description=Embedding parallel gradient synchronization,group_kind=ep,input__shape=[12576, 20480],input__dtype=torch.float16,timestamp=3587825928.51,sub_operations=[])
rank:8064:optimizer_step(stage_id=63,batch_id=0,mg_state=None,duration=27.96,description=simulation,group_kind=None,input__shape=None,input__dtype=None,timestamp=3587825956.5,sub_operations=[])
